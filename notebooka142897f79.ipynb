{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":8870083,"sourceType":"datasetVersion","datasetId":5338273}],"dockerImageVersionId":30746,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport plotly.figure_factory as ff\nfrom textblob import TextBlob\nimport numpy as np\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-16T11:39:04.788476Z","iopub.execute_input":"2024-07-16T11:39:04.789001Z","iopub.status.idle":"2024-07-16T11:39:04.798593Z","shell.execute_reply.started":"2024-07-16T11:39:04.788958Z","shell.execute_reply":"2024-07-16T11:39:04.797021Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the data\npath = '/kaggle/input/sentiment-analysis-for-mental-health/Combined Data.csv'\ndf = pd.read_csv(path)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:40.70492Z","iopub.execute_input":"2024-07-16T07:36:40.705539Z","iopub.status.idle":"2024-07-16T07:36:41.181949Z","shell.execute_reply.started":"2024-07-16T07:36:40.705504Z","shell.execute_reply":"2024-07-16T07:36:41.180717Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Display the first few rows of the dataframe\nprint(df.head())","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:41.183535Z","iopub.execute_input":"2024-07-16T07:36:41.184024Z","iopub.status.idle":"2024-07-16T07:36:41.197313Z","shell.execute_reply.started":"2024-07-16T07:36:41.183984Z","shell.execute_reply":"2024-07-16T07:36:41.195868Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# EDA\nprint(\"Dataset Info:\")\nprint(df.info())","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:41.200997Z","iopub.execute_input":"2024-07-16T07:36:41.201398Z","iopub.status.idle":"2024-07-16T07:36:41.216935Z","shell.execute_reply.started":"2024-07-16T07:36:41.201367Z","shell.execute_reply":"2024-07-16T07:36:41.215568Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Missing Values:\")\nprint(df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:41.219231Z","iopub.execute_input":"2024-07-16T07:36:41.219654Z","iopub.status.idle":"2024-07-16T07:36:41.23989Z","shell.execute_reply.started":"2024-07-16T07:36:41.219623Z","shell.execute_reply":"2024-07-16T07:36:41.238751Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Distribution of target labels\nfig = px.histogram(df, x='status', title='Distribution of Mental Health Status')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:41.241373Z","iopub.execute_input":"2024-07-16T07:36:41.241758Z","iopub.status.idle":"2024-07-16T07:36:42.015239Z","shell.execute_reply.started":"2024-07-16T07:36:41.241726Z","shell.execute_reply":"2024-07-16T07:36:42.013958Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Handle NaN values in the statement column\ndf['statement'] = df['statement'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:42.016767Z","iopub.execute_input":"2024-07-16T07:36:42.017149Z","iopub.status.idle":"2024-07-16T07:36:42.159053Z","shell.execute_reply.started":"2024-07-16T07:36:42.017117Z","shell.execute_reply":"2024-07-16T07:36:42.157822Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Text Length Distribution\ndf['text_length'] = df['statement'].apply(lambda x: len(str(x).split()))\nfig = px.histogram(df, x='text_length', title='Text Length Distribution')\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:42.160724Z","iopub.execute_input":"2024-07-16T07:36:42.161172Z","iopub.status.idle":"2024-07-16T07:36:42.169636Z","shell.execute_reply.started":"2024-07-16T07:36:42.161132Z","shell.execute_reply":"2024-07-16T07:36:42.168528Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Preprocessing\nnltk.download('stopwords')\nnltk.download('punkt')\n\ndef preprocess_text(text):\n    text = text.lower()  # Lowercase text\n    text = re.sub(r'\\[.*?\\]', '', text)  # Remove text in square brackets\n    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)  # Remove links\n    text = re.sub(r'<.*?>+', '', text)  # Remove HTML tags\n    text = re.sub(r'[%s]' % re.escape(string.punctuation), '', text)  # Remove punctuation\n    text = re.sub(r'\\n', '', text)  # Remove newlines\n    text = re.sub(r'\\w*\\d\\w*', '', text)  # Remove words containing numbers\n    return text\n\ndf['cleaned_statement'] = df['statement'].apply(lambda x: preprocess_text(x))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:42.171196Z","iopub.execute_input":"2024-07-16T07:36:42.171681Z","iopub.status.idle":"2024-07-16T07:36:44.236741Z","shell.execute_reply.started":"2024-07-16T07:36:42.171643Z","shell.execute_reply":"2024-07-16T07:36:44.233382Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenization and Stopwords Removal\nstop_words = set(stopwords.words('english'))\n\ndef remove_stopwords(text):\n    tokens = word_tokenize(text)\n    tokens = [word for word in tokens if word not in stop_words]\n    return ' '.join(tokens)\n\ndf['cleaned_statement'] = df['cleaned_statement'].apply(lambda x: remove_stopwords(x))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:44.238121Z","iopub.status.idle":"2024-07-16T07:36:44.238702Z","shell.execute_reply.started":"2024-07-16T07:36:44.238413Z","shell.execute_reply":"2024-07-16T07:36:44.238436Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Augmentation\ndef augment_text(text):\n    try:\n        blob = TextBlob(text)\n        translated = blob.translate(to='fr').translate(to='en')\n        return str(translated)\n    except Exception as e:\n        return text\n\ndf['augmented_statement'] = df['statement'].apply(augment_text)\naugmented_df = df[['statement', 'status']].copy()\naugmented_df['statement'] = df['augmented_statement']\ndf = pd.concat([df, augmented_df])","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:44.240012Z","iopub.status.idle":"2024-07-16T07:36:44.240585Z","shell.execute_reply.started":"2024-07-16T07:36:44.240296Z","shell.execute_reply":"2024-07-16T07:36:44.240321Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reapply preprocessing on augmented data\ndf['cleaned_statement'] = df['statement'].apply(lambda x: preprocess_text(x))\ndf['cleaned_statement'] = df['cleaned_statement'].apply(lambda x: remove_stopwords(x))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:44.242338Z","iopub.status.idle":"2024-07-16T07:36:44.242825Z","shell.execute_reply.started":"2024-07-16T07:36:44.242619Z","shell.execute_reply":"2024-07-16T07:36:44.242637Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Ensure no NaN values are left\ndf['cleaned_statement'] = df['cleaned_statement'].fillna('')","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:44.244152Z","iopub.status.idle":"2024-07-16T07:36:44.244618Z","shell.execute_reply.started":"2024-07-16T07:36:44.244368Z","shell.execute_reply":"2024-07-16T07:36:44.244393Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Splitting the data\nX = df['cleaned_statement']\ny = df['status']","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:44.245827Z","iopub.status.idle":"2024-07-16T07:36:44.24627Z","shell.execute_reply.started":"2024-07-16T07:36:44.246023Z","shell.execute_reply":"2024-07-16T07:36:44.246038Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:44.249087Z","iopub.status.idle":"2024-07-16T07:36:44.249748Z","shell.execute_reply.started":"2024-07-16T07:36:44.249405Z","shell.execute_reply":"2024-07-16T07:36:44.24943Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vectorization\nvectorizer = TfidfVectorizer(max_features=10000)  \nX_train_tfidf = vectorizer.fit_transform(X_train)\nX_test_tfidf = vectorizer.transform(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:44.253603Z","iopub.status.idle":"2024-07-16T07:36:44.254127Z","shell.execute_reply.started":"2024-07-16T07:36:44.253862Z","shell.execute_reply":"2024-07-16T07:36:44.253883Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Model Training with Hyperparameter Tuning\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100]\n}\n\nmodel = LogisticRegression(max_iter=1000)\ngrid_search = GridSearchCV(model, param_grid, cv=5, scoring='accuracy')\ngrid_search.fit(X_train_tfidf, y_train)\n\n# Best Model\nbest_model = grid_search.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:44.255768Z","iopub.status.idle":"2024-07-16T07:36:44.256309Z","shell.execute_reply.started":"2024-07-16T07:36:44.25603Z","shell.execute_reply":"2024-07-16T07:36:44.256053Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Predictions\ny_pred = best_model.predict(X_test_tfidf)","metadata":{"execution":{"iopub.status.busy":"2024-07-16T07:36:44.257954Z","iopub.status.idle":"2024-07-16T07:36:44.258502Z","shell.execute_reply.started":"2024-07-16T07:36:44.258206Z","shell.execute_reply":"2024-07-16T07:36:44.258229Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluation\nprint(\"Best Parameters:\")\nprint(grid_search.best_params_)\n\nprint(\"Accuracy Score:\")\nprint(accuracy_score(y_test, y_pred))\n\nprint(\"Classification Report:\")\nprint(classification_report(y_test, y_pred))","metadata":{"execution":{"iopub.status.busy":"2024-07-16T10:11:06.411663Z","iopub.execute_input":"2024-07-16T10:11:06.412672Z","iopub.status.idle":"2024-07-16T10:46:42.392924Z","shell.execute_reply.started":"2024-07-16T10:11:06.412615Z","shell.execute_reply":"2024-07-16T10:46:42.39169Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Confusion Matrix\ncm = confusion_matrix(y_test, y_pred)\ncm_fig = ff.create_annotated_heatmap(\n    z=cm,\n    x=list(set(y_test)),\n    y=list(set(y_test)),\n    annotation_text=cm,\n    colorscale='Viridis'\n)\ncm_fig.update_layout(title='Confusion Matrix')\ncm_fig.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Feature Importance\nfeature_names = vectorizer.get_feature_names_out()\ncoefs = best_model.coef_\nfor i, category in enumerate(best_model.classes_):\n    top_features = coefs[i].argsort()[-10:]\n    top_words = [feature_names[j] for j in top_features]\n    top_scores = [coefs[i][j] for j in top_features]\n    fig = go.Figure([go.Bar(x=top_words, y=top_scores)])\n    fig.update_layout(title=f'Top Features for {category}')\n    fig.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n# Word Cloud\nall_text = ' '.join(df['cleaned_statement'])\nwordcloud = WordCloud(width=800, height=400, background_color='white').generate(all_text)\nplt.figure(figsize=(10, 5))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.title('Word Cloud of Cleaned Statements')\nplt.show()","metadata":{},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Status Distribution\nfig = px.pie(df, names='status', title='Proportion of Each Status Category')\nfig.show()","metadata":{},"outputs":[],"execution_count":null}]}